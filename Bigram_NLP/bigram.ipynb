{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### ***Name : Ashuwin P***\n",
    "##### ***Reg.No : 3122 22 5002 013***\n",
    "##### ***Course : UIT2622 ~ Advanced Artificial Intelligence Techniques***\n",
    "##### ***Topic : Language Models ~ Implementation of Bigram model for word prediction***\n",
    "##### ***Last Update : 05 November 2024***\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>***Aim***</u>\n",
    "The aim of this project is to develop a <b>bigram model</b> for predicting the next word in a sequence of text using natural language processing (NLP) techniques. The model will be built using a corpus of text, which will be preprocessed to generate bigrams and their corresponding probabilities for accurate next-word prediction.\n",
    "\n",
    "### <u>***Introduction***</u>\n",
    "Natural Language Processing (NLP) involves the interaction between computers and humans through natural language. One of the essential tasks in NLP is language modeling, which helps in understanding and predicting the next words in a sentence. A bigram model is a type of language model that uses pairs of consecutive words (bigrams) to make predictions. This project focuses on implementing a bigram-based next-word prediction system by processing a given corpus of text.\n",
    "\n",
    "### <u>***Overview***</u>\n",
    "The implemented model consists of several key components:\n",
    "- ***Corpus Reading:*** Reading and loading the text data from a specified file.\n",
    "- ***Preprocessing:*** Converting the text to lowercase, removing punctuation, and marking the end of sentences.\n",
    "- ***Tokenization:*** Splitting the text into individual tokens (words).\n",
    "- ***Frequency Calculation:*** Counting the occurrences of each token and bigram in the text.\n",
    "- ***Probability Calculation:*** Computing the conditional probabilities of each word following another word using bigrams.\n",
    "- ***Prediction:*** Using the bigram probabilities to suggest the next word based on the last word entered by the user.\n",
    "\n",
    "### <u>***Methodology***</u>\n",
    "The methodology for implementing the bigram model includes the following steps:\n",
    "1. ***Read the Corpus:*** The text data is read from a file and stored in a string format.\n",
    "2. ***Preprocess the Text:*** The text is preprocessed to remove punctuation and convert it to lowercase. An end-of-sentence marker is added for clarity.\n",
    "3. ***Tokenization:*** The preprocessed text is split into tokens for further analysis.\n",
    "4. ***Frequency Calculation:*** A frequency count of individual tokens and bigrams is performed to facilitate probability calculations.\n",
    "5. ***Probability Calculation:*** Conditional probabilities are calculated using the frequency of bigrams and individual tokens.\n",
    "6. ***Next Word Prediction:*** Given an input phrase, the model predicts the next word based on the calculated probabilities.\n",
    "\n",
    "### <u>***Formulae***</u>\n",
    "The following formula is used to calculate the conditional probability of a word given the previous word in the bigram model:\n",
    "$$\n",
    "P(w_2 | w_1) = \\frac{C(w_1, w_2)}{C(w_1)}\n",
    "$$\n",
    "where:\n",
    "- \\( P(w_2 | w_1) \\) is the probability of word \\( w_2 \\) given word \\( w_1 \\).\n",
    "- \\( C(w_1, w_2) \\) is the count of the bigram \\( (w_1, w_2) \\) in the corpus.\n",
    "- \\( C(w_1) \\) is the count of the word \\( w_1 \\) in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The implementation uses the Python programming language and the `collections` module to manage frequencies and probabilities. Key functions include:\n",
    "- **read_corpus(file_path):** Reads the text data from a specified file path.\n",
    "- **preprocess(text):** Cleans and prepares the text for tokenization.\n",
    "- **generate_tokens(text):** Splits the text into individual tokens (words).\n",
    "- **generate_token_frequencies(tokens):** Counts the frequency of each token in the list of tokens.\n",
    "- **generate_bigrams(tokens):** Creates a list of bigrams from the tokens.\n",
    "- **generate_bigram_freq(bigrams):** Calculates the frequency of each bigram.\n",
    "- **build_bigram_probabilities(bigram_freq, token_freq):** Computes conditional probabilities for bigrams based on token frequencies.\n",
    "- **predict_next_word(input_text, bigram_probs):** Provides suggestions for the next word based on user input.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read text from file\n",
    "def read_corpus(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing and Tokenizing functions\n",
    "def preprocess(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Replace period with end-of-sentence marker\n",
    "    text = text.replace(\".\", \" eos\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tokens(text):\n",
    "    # Split text on whitespace and remove any remaining punctuation\n",
    "    tokens = text.split()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_token_frequencies(tokens): \n",
    "    token_freq = {} \n",
    "    for token in tokens: \n",
    "        if token in token_freq: \n",
    "            token_freq[token] += 1 \n",
    "        else: \n",
    "            token_freq[token] = 1 \n",
    "    return token_freq "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bigrams(tokens): \n",
    "    return [(tokens[i], tokens[i + 1]) for i in range(len(tokens) - 1)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bigram_freq(bigrams): \n",
    "    freq_dict = defaultdict(int) \n",
    "    for bigram in bigrams: \n",
    "        freq_dict[\" \".join(bigram)] += 1 \n",
    "    return freq_dict "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"D:\\SEM5\\AAIT\\Practical\\Bigram_NLP\\img1.png\" alt=\"Formula\" width=\"300\" height=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bigram_probabilities(bigram_freq, token_freq): \n",
    "    bigram_probs = defaultdict(dict) \n",
    "    for bigram, count in bigram_freq.items(): \n",
    "        word1, word2 = bigram.split() \n",
    "        if token_freq[word1] > 0:  # Ensure we do not divide by zero\n",
    "            bigram_probs[word1][word2] = count / token_freq[word1] \n",
    "    return bigram_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(input_text, bigram_probs): \n",
    "    tokens = generate_tokens(preprocess(input_text)) \n",
    "    last_word = tokens[-1] if tokens else \"\"\n",
    "    if last_word not in bigram_probs: \n",
    "        return \"No suggestions available.\" \n",
    "\n",
    "    next_word_candidates = sorted(bigram_probs[last_word].items(), key=lambda x: x[1], reverse=True) \n",
    "    suggestions = [word for word, prob in next_word_candidates[:5]]  # Top 5 suggestions \n",
    "    return suggestions if suggestions else \"No suggestions available.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_some(dictionary, n):\n",
    "    for i, (key, value) in enumerate(dictionary.items()):\n",
    "        if i < n:\n",
    "            print(f\"{key}: {value}\")\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"D:\\SEM5\\AAIT\\Practical\\Bigram_NLP\\NLP_Corpus.txt\"\n",
    "corpus_text = read_corpus(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_text = preprocess(corpus_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = generate_tokens(preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', 'nlp', 'defined', 'natural', 'language', 'processing', 'nlp', 'is', 'a', 'branch', 'of', 'artificial', 'intelligence', 'ai', 'that', 'enables', 'computers', 'to']\n"
     ]
    }
   ],
   "source": [
    "print(tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_freq = generate_token_frequencies(tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "natural: 14\n",
      "language: 20\n",
      "processing: 15\n",
      "nlp: 55\n",
      "defined: 1\n",
      "is: 56\n",
      "a: 74\n",
      "branch: 2\n",
      "of: 84\n",
      "artificial: 2\n"
     ]
    }
   ],
   "source": [
    "print_some(token_freq, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = generate_bigrams(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('natural', 'language'), ('language', 'processing'), ('processing', 'nlp'), ('nlp', 'defined'), ('defined', 'natural'), ('natural', 'language'), ('language', 'processing'), ('processing', 'nlp'), ('nlp', 'is'), ('is', 'a')]\n"
     ]
    }
   ],
   "source": [
    "print(bigrams[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_freq = generate_bigram_freq(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "natural language: 13\n",
      "language processing: 8\n",
      "processing nlp: 3\n",
      "nlp defined: 1\n",
      "defined natural: 1\n",
      "nlp is: 6\n",
      "is a: 16\n",
      "a branch: 2\n",
      "branch of: 2\n",
      "of artificial: 2\n"
     ]
    }
   ],
   "source": [
    "print_some(bigram_freq, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate probability table for bigram model\n",
    "bigram_probs = build_bigram_probabilities(bigram_freq, token_freq) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "natural: {'language': 0.9285714285714286, 'sentences': 0.07142857142857142}\n",
      "language: {'processing': 0.4, 'natural': 0.05, 'text': 0.05, 'nlp': 0.05, 'understanding': 0.05, 'generation': 0.05, 'respectively': 0.05, 'while': 0.05, 'research': 0.05, 'from': 0.05, 'of': 0.05, 'typically': 0.05, 'is': 0.05}\n",
      "processing: {'nlp': 0.2, 'has': 0.06666666666666667, 'automate': 0.06666666666666667, 'it': 0.06666666666666667, 'pipeline': 0.06666666666666667, 'they': 0.13333333333333333, 'time': 0.06666666666666667, 'large': 0.06666666666666667, 'library': 0.06666666666666667, 'pipelines': 0.06666666666666667, 'is': 0.06666666666666667, 'will': 0.06666666666666667}\n"
     ]
    }
   ],
   "source": [
    "print_some(bigram_probs, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Type a phrase:  NLP\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggestions for the next word: ['is', 'can', 'models', 'to', 'technology']\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_text = input(\"Type a phrase: \") \n",
    "suggestions = predict_next_word(input_text, bigram_probs) \n",
    "print(\"Suggestions for the next word:\", suggestions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>***Conclusion***</u>\n",
    "The bigram model implemented in this project demonstrates the basic principles of next-word prediction using natural language processing techniques. By utilizing bigrams, the model can effectively suggest the next word based on the previous word, thus enabling a simple yet functional language prediction system. This project serves as a foundational step towards understanding more complex language models and their applications in various NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
